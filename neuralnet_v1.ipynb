{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assgnment1_DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVjuXsr1zLu2"
      },
      "source": [
        "#import the dataset \n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQLWjHat0lV4"
      },
      "source": [
        "#import the required libraries \n",
        "from matplotlib import pyplot\n",
        "import numpy as np"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP17UY7v0C7w"
      },
      "source": [
        "#load the data into train and test \n",
        "(X_train,y_train),(X_test,y_test)=fashion_mnist.load_data()\n",
        "#pyplot.imshow(X_train[2])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z27sdlQmMH1O",
        "outputId": "24480500-1f7d-4219-a65c-76e043c3c77b"
      },
      "source": [
        "noOfImages = X_train.shape[0]\n",
        "X_train = (1.0/255)*np.array([X_train[i].flatten() for i in range(0,noOfImages)])\n",
        "X_train = np.array([X_train[i].flatten() for i in range(0,noOfImages)])\n",
        "noOftestImages= X_test.shape[0]\n",
        "X_test =(1.0/255)*np.array([X_test[i].flatten() for i in range(0,noOftestImages)])\n",
        "X_test =np.array([X_test[i].flatten() for i in range(0,noOftestImages)])\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtQEmgi3Opy_",
        "outputId": "7255d5e6-eb7e-464b-8a1c-7485fb8c1a0a"
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
            " 0.         0.00392157 0.01568627 0.         0.         0.\n",
            " 0.         0.00392157 0.00392157 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.01176471 0.\n",
            " 0.14117647 0.53333333 0.49803922 0.24313725 0.21176471 0.\n",
            " 0.         0.         0.00392157 0.01176471 0.01568627 0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.02352941 0.         0.4        0.8\n",
            " 0.69019608 0.5254902  0.56470588 0.48235294 0.09019608 0.\n",
            " 0.         0.         0.         0.04705882 0.03921569 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
            " 0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
            " 0.30196078 0.50980392 0.28235294 0.05882353 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00392157 0.         0.27058824\n",
            " 0.81176471 0.8745098  0.85490196 0.84705882 0.84705882 0.63921569\n",
            " 0.49803922 0.4745098  0.47843137 0.57254902 0.55294118 0.34509804\n",
            " 0.6745098  0.25882353 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.00392157 0.00392157 0.         0.78431373 0.90980392 0.90980392\n",
            " 0.91372549 0.89803922 0.8745098  0.8745098  0.84313725 0.83529412\n",
            " 0.64313725 0.49803922 0.48235294 0.76862745 0.89803922 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
            " 0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
            " 0.8745098  0.96078431 0.67843137 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.75686275\n",
            " 0.89411765 0.85490196 0.83529412 0.77647059 0.70588235 0.83137255\n",
            " 0.82352941 0.82745098 0.83529412 0.8745098  0.8627451  0.95294118\n",
            " 0.79215686 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.01176471 0.         0.04705882 0.85882353 0.8627451  0.83137255\n",
            " 0.85490196 0.75294118 0.6627451  0.89019608 0.81568627 0.85490196\n",
            " 0.87843137 0.83137255 0.88627451 0.77254902 0.81960784 0.20392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.02352941 0.\n",
            " 0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
            " 0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
            " 0.96078431 0.46666667 0.65490196 0.21960784 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.01568627 0.         0.         0.21568627 0.9254902\n",
            " 0.89411765 0.90196078 0.89411765 0.94117647 0.90980392 0.83529412\n",
            " 0.85490196 0.8745098  0.91764706 0.85098039 0.85098039 0.81960784\n",
            " 0.36078431 0.         0.         0.         0.00392157 0.01568627\n",
            " 0.02352941 0.02745098 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.92941176 0.88627451 0.85098039 0.8745098\n",
            " 0.87058824 0.85882353 0.87058824 0.86666667 0.84705882 0.8745098\n",
            " 0.89803922 0.84313725 0.85490196 1.         0.30196078 0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.24313725 0.56862745 0.8\n",
            " 0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
            " 0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
            " 0.87843137 0.95686275 0.62352941 0.         0.         0.\n",
            " 0.         0.         0.07058824 0.17254902 0.32156863 0.41960784\n",
            " 0.74117647 0.89411765 0.8627451  0.87058824 0.85098039 0.88627451\n",
            " 0.78431373 0.80392157 0.82745098 0.90196078 0.87843137 0.91764706\n",
            " 0.69019608 0.7372549  0.98039216 0.97254902 0.91372549 0.93333333\n",
            " 0.84313725 0.         0.         0.22352941 0.73333333 0.81568627\n",
            " 0.87843137 0.86666667 0.87843137 0.81568627 0.8        0.83921569\n",
            " 0.81568627 0.81960784 0.78431373 0.62352941 0.96078431 0.75686275\n",
            " 0.80784314 0.8745098  1.         1.         0.86666667 0.91764706\n",
            " 0.86666667 0.82745098 0.8627451  0.90980392 0.96470588 0.\n",
            " 0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
            " 0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
            " 0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
            " 0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
            " 0.87058824 0.89411765 0.88235294 0.         0.38431373 0.91372549\n",
            " 0.77647059 0.82352941 0.87058824 0.89803922 0.89803922 0.91764706\n",
            " 0.97647059 0.8627451  0.76078431 0.84313725 0.85098039 0.94509804\n",
            " 0.25490196 0.28627451 0.41568627 0.45882353 0.65882353 0.85882353\n",
            " 0.86666667 0.84313725 0.85098039 0.8745098  0.8745098  0.87843137\n",
            " 0.89803922 0.11372549 0.29411765 0.8        0.83137255 0.8\n",
            " 0.75686275 0.80392157 0.82745098 0.88235294 0.84705882 0.7254902\n",
            " 0.77254902 0.80784314 0.77647059 0.83529412 0.94117647 0.76470588\n",
            " 0.89019608 0.96078431 0.9372549  0.8745098  0.85490196 0.83137255\n",
            " 0.81960784 0.87058824 0.8627451  0.86666667 0.90196078 0.2627451\n",
            " 0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
            " 0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
            " 0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
            " 0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
            " 0.70980392 0.80392157 0.80784314 0.45098039 0.         0.47843137\n",
            " 0.85882353 0.75686275 0.70196078 0.67058824 0.71764706 0.76862745\n",
            " 0.8        0.82352941 0.83529412 0.81176471 0.82745098 0.82352941\n",
            " 0.78431373 0.76862745 0.76078431 0.74901961 0.76470588 0.74901961\n",
            " 0.77647059 0.75294118 0.69019608 0.61176471 0.65490196 0.69411765\n",
            " 0.82352941 0.36078431 0.         0.         0.29019608 0.74117647\n",
            " 0.83137255 0.74901961 0.68627451 0.6745098  0.68627451 0.70980392\n",
            " 0.7254902  0.7372549  0.74117647 0.7372549  0.75686275 0.77647059\n",
            " 0.8        0.81960784 0.82352941 0.82352941 0.82745098 0.7372549\n",
            " 0.7372549  0.76078431 0.75294118 0.84705882 0.66666667 0.\n",
            " 0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
            " 0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
            " 0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
            " 0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
            " 0.38823529 0.22745098 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.15686275\n",
            " 0.23921569 0.17254902 0.28235294 0.16078431 0.1372549  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFA10HAHKCb8"
      },
      "source": [
        "def softmax(X):\n",
        "  X=np.exp(X)\n",
        "  sum=np.sum(X,axis=0)\n",
        "  return X/sum \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZgtMpEAUkIA"
      },
      "source": [
        "def sigmoidFunc(X):\n",
        "\n",
        "  return 1.0/(1.0+np.exp(-X))\n",
        "  #return res"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GJHo_F55wKD"
      },
      "source": [
        "def gDash(X):\n",
        "  return sigmoidFunc(X)*(1-sigmoidFunc(X))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIfFadrLAkmM"
      },
      "source": [
        "def forwardPropagation(X,parameters):\n",
        "  preactivation={}\n",
        "  activation={}\n",
        "  activation['h0']=X.T\n",
        "  #print(activation['h0'].shape)\n",
        "  for k in range(1,noOfHiddenLayers+1):\n",
        "    preactivation['a'+str(k)]=np.dot(parameters['W'+str(k)],activation['h'+str(k-1)])+parameters['b'+str(k)]\n",
        "    activation['h'+str(k)]=sigmoidFunc(preactivation['a'+str(k)])\n",
        "    #print('h size '+str(k),activation['h'+str(k)].shape)\n",
        "    #print('a'+str(k),preactivation['a'+str(k)])\n",
        "  preactivation['a'+str(noOfHiddenLayers+1)]=np.dot(parameters['W'+str(noOfHiddenLayers+1)],activation['h'+str(noOfHiddenLayers)])+parameters['b'+str(noOfHiddenLayers+1)]\n",
        "  y=softmax(preactivation['a'+str(noOfHiddenLayers+1)])    \n",
        "  #print(\"a last\" ,preactivation['a'+str(noOfHiddenLayers+1)])\n",
        "  return (preactivation,activation,y)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZFsUg0xrJtv"
      },
      "source": [
        "def backPropagation(parameters,activation,preactivation,yhat,X,y_train):\n",
        "  grads={}\n",
        "  eIndicator=np.zeros((10,X.shape[0]))\n",
        "  eIndicator[y_train,np.arange(X.shape[0])]=1\n",
        "  #print(y_train)\n",
        "  #print(eIndicator)\n",
        "  #print(eIndicator.shape)\n",
        "  grads['a'+str(noOfHiddenLayers+1)]= -(eIndicator-yhat)\n",
        "  #print(grads['a'+str(noOfHiddenLayers+1)].shape)\n",
        "  for j in range(noOfHiddenLayers+1,0,-1):\n",
        "    grads['W'+str(j)]= np.dot(grads['a'+str(j)],activation['h'+str(j-1)].T)\n",
        "    #grads['W'+str(j)]= np.dot(activation['h'+str(j-1)],grads['a'+str(j)].T).T\n",
        "    #print(grads['W'+str(j)].shape)\n",
        "    grads['b'+str(j)]= np.sum(grads['a'+str(j)],axis=1,keepdims=True)\n",
        "    ###grads['h'+str(j-1)]=np.dot(grads['W'+str(j)].T,grads['a'+str(j)]) #error\n",
        "    grads['h'+str(j-1)]=np.dot(parameters['W'+str(j)].T,grads['a'+str(j)])\n",
        "    #print('h'+str(j-1),grads['h'+str(j-1)].shape)\n",
        "    #print('a'+str(j-1),preactivation['a'+str(j-1)].shape)\n",
        "    if (j!=1):\n",
        "      grads['a'+str(j-1)]=grads['h'+str(j-1)]*gDash(preactivation['a'+str(j-1)])\n",
        "  return grads\n",
        "#backPropagation()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MXQ17Gk_Cuv"
      },
      "source": [
        "def Loss(yhat,y_train,X):\n",
        "  eIndicator=np.zeros((10,X.shape[0]))\n",
        "  eIndicator[y_train,np.arange(X.shape[0])]=1\n",
        "  eIndicator=eIndicator*yhat\n",
        "  eIndicator=eIndicator.sum(axis=0)\n",
        "  eIndicator=np.log(eIndicator)\n",
        "  return -sum(eIndicator)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnJHWPgMiiLw",
        "outputId": "c7870bc1-8497-4ee2-da3d-243d46d689aa"
      },
      "source": [
        "noOfHiddenLayers=2\n",
        "l=10 #output classes\n",
        "noOfneuronsEach=[120,20]\n",
        "inputNeuronSize=X_train.shape[1]\n",
        "print(inputNeuronSize)\n",
        "parameters={}\n",
        "eta=1\n",
        "#eta=0.1\n",
        "#input W\n",
        "def gradDescent():\n",
        "  #batchSize=1000\n",
        "  iterations=300\n",
        "  #initialization\n",
        "  parameters['W'+str(1)] =np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[0], inputNeuronSize))\n",
        "  parameters['b'+str(1)]= np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[0],1))\n",
        "  #parameters['W'+str(1)] =0+1.5*np.random.randn(noOfneuronsEach[0], inputNeuronSize)\n",
        "  #parameters['b'+str(1)]= 0+1.5*np.random.randn(noOfneuronsEach[0],1)\n",
        "  for i in range(2,noOfHiddenLayers+1):\n",
        "    parameters['W'+str(i)] = np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "    parameters['b'+str(i)]= np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[i-1],1))\n",
        "    #parameters['W'+str(i)] = 0+1.5*np.random.randn(noOfneuronsEach[i-1], noOfneuronsEach[i-2])\n",
        "    #parameters['b'+str(i)]= 0+1.5*np.random.randn(noOfneuronsEach[i-1],1)\n",
        "  #Output W\n",
        "  parameters['W'+str(noOfHiddenLayers+1)] = np.random.uniform(low=-0.5,high=0.5,size=(l, noOfneuronsEach[-1]))\n",
        "  parameters['b'+str(noOfHiddenLayers+1)]= np.random.uniform(low=-0.5,high=0.5,size=(l,1))\n",
        "  #parameters['W'+str(noOfHiddenLayers+1)] = 0+1.5*np.random.randn(l, noOfneuronsEach[-1])\n",
        "  #parameters['b'+str(noOfHiddenLayers+1)]= 0+1.5*np.random.randn(l,1)\n",
        "  t=0\n",
        "  while(t < iterations):\n",
        "    print(\"iter \",t)\n",
        "    #print(parameters)\n",
        "    #mini=0\n",
        "    #while(mini<(noOfImages/batchSize)):\n",
        "      #print(\"Epoch\",mini)\n",
        "      #X_mini=X_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      #y_mini=y_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "    preactivation,activation,yhat=forwardPropagation(X_train,parameters)\n",
        "      #print(preactivation)\n",
        "      #print(activation)\n",
        "    #print(\"y_pred\",yhat)\n",
        "    print(\"loss\",Loss(yhat,y_train,X_train))\n",
        "    gradients=backPropagation(parameters,activation,preactivation,yhat,X_train,y_train)\n",
        "    #print(\"gradients\",gradients)\n",
        "    for i in range(1,noOfHiddenLayers+2):\n",
        "      parameters['W'+str(i)] -=  eta*(1.0/X_train.shape[0])*gradients['W'+str(i)]#parameters['W'+str(i)] -=  eta*(1.0/X_train.shape[0])*gradients['W'+str(i)]\n",
        "      parameters['b'+str(i)] -= eta*(1.0/X_train.shape[0])*gradients['b'+str(i)]#parameters['b'+str(i)] -= eta*(1.0/X_train.shape[0])*gradients['b'+str(i)]\n",
        "    #mini+=1\n",
        "    #_,_,yhat=forwardPropagation(X_train,parameters)\n",
        "    \n",
        "    #print(yhat)\n",
        "    t+=1\n",
        "gradDescent()\n",
        "print(parameters)\n",
        "#print(gradients)\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "784\n",
            "iter  0\n",
            "loss -154997.02400733135\n",
            "iter  1\n",
            "loss -137447.60304714643\n",
            "iter  2\n",
            "loss -132578.50028885275\n",
            "iter  3\n",
            "loss -129888.84761075344\n",
            "iter  4\n",
            "loss -127316.21533296791\n",
            "iter  5\n",
            "loss -124616.48549251058\n",
            "iter  6\n",
            "loss -121751.40315815431\n",
            "iter  7\n",
            "loss -118723.3530597767\n",
            "iter  8\n",
            "loss -115579.41768477685\n",
            "iter  9\n",
            "loss -112403.75758766153\n",
            "iter  10\n",
            "loss -109279.60108687232\n",
            "iter  11\n",
            "loss -106255.88530774502\n",
            "iter  12\n",
            "loss -103351.48096244877\n",
            "iter  13\n",
            "loss -100573.8746888065\n",
            "iter  14\n",
            "loss -97926.58170678712\n",
            "iter  15\n",
            "loss -95409.56434573549\n",
            "iter  16\n",
            "loss -93019.10343926252\n",
            "iter  17\n",
            "loss -90748.53876088017\n",
            "iter  18\n",
            "loss -88589.73089854687\n",
            "iter  19\n",
            "loss -86534.6106756568\n",
            "iter  20\n",
            "loss -84576.15167333823\n",
            "iter  21\n",
            "loss -82708.59830842337\n",
            "iter  22\n",
            "loss -80927.25922864968\n",
            "iter  23\n",
            "loss -79228.23786627903\n",
            "iter  24\n",
            "loss -77608.23225852141\n",
            "iter  25\n",
            "loss -76064.33253845594\n",
            "iter  26\n",
            "loss -74593.75658353124\n",
            "iter  27\n",
            "loss -73193.59163084727\n",
            "iter  28\n",
            "loss -71860.65922852671\n",
            "iter  29\n",
            "loss -70591.54279930856\n",
            "iter  30\n",
            "loss -69382.7176697366\n",
            "iter  31\n",
            "loss -68230.69595295923\n",
            "iter  32\n",
            "loss -67132.13281456642\n",
            "iter  33\n",
            "loss -66083.88287161235\n",
            "iter  34\n",
            "loss -65083.01787212496\n",
            "iter  35\n",
            "loss -64126.8212223716\n",
            "iter  36\n",
            "loss -63212.77148802993\n",
            "iter  37\n",
            "loss -62338.522346953905\n",
            "iter  38\n",
            "loss -61501.88295921687\n",
            "iter  39\n",
            "loss -60700.80054914578\n",
            "iter  40\n",
            "loss -59933.34577973781\n",
            "iter  41\n",
            "loss -59197.70086690328\n",
            "iter  42\n",
            "loss -58492.15008339529\n",
            "iter  43\n",
            "loss -57815.072196011024\n",
            "iter  44\n",
            "loss -57164.93437994385\n",
            "iter  45\n",
            "loss -56540.28720893563\n",
            "iter  46\n",
            "loss -55939.76039636044\n",
            "iter  47\n",
            "loss -55362.05904076045\n",
            "iter  48\n",
            "loss -54805.96019918237\n",
            "iter  49\n",
            "loss -54270.309668464055\n",
            "iter  50\n",
            "loss -53754.018897990914\n",
            "iter  51\n",
            "loss -53256.0619887465\n",
            "iter  52\n",
            "loss -52775.4727549886\n",
            "iter  53\n",
            "loss -52311.34183891699\n",
            "iter  54\n",
            "loss -51862.81387726574\n",
            "iter  55\n",
            "loss -51429.08472357721\n",
            "iter  56\n",
            "loss -51009.39873222518\n",
            "iter  57\n",
            "loss -50603.046111030686\n",
            "iter  58\n",
            "loss -50209.36034916112\n",
            "iter  59\n",
            "loss -49827.715726370676\n",
            "iter  60\n",
            "loss -49457.52490879524\n",
            "iter  61\n",
            "loss -49098.23663554136\n",
            "iter  62\n",
            "loss -48749.33349940138\n",
            "iter  63\n",
            "loss -48410.32982410224\n",
            "iter  64\n",
            "loss -48080.7696396491\n",
            "iter  65\n",
            "loss -47760.22475654285\n",
            "iter  66\n",
            "loss -47448.2929389241\n",
            "iter  67\n",
            "loss -47144.59617604663\n",
            "iter  68\n",
            "loss -46848.77905088911\n",
            "iter  69\n",
            "loss -46560.507204216854\n",
            "iter  70\n",
            "loss -46279.46589190502\n",
            "iter  71\n",
            "loss -46005.35863299162\n",
            "iter  72\n",
            "loss -45737.90594551437\n",
            "iter  73\n",
            "loss -45476.844166944175\n",
            "iter  74\n",
            "loss -45221.9243557399\n",
            "iter  75\n",
            "loss -44972.911270342054\n",
            "iter  76\n",
            "loss -44729.58242177791\n",
            "iter  77\n",
            "loss -44491.727195920066\n",
            "iter  78\n",
            "loss -44259.146041351\n",
            "iter  79\n",
            "loss -44031.64971880441\n",
            "iter  80\n",
            "loss -43809.05860813059\n",
            "iter  81\n",
            "loss -43591.20206881603\n",
            "iter  82\n",
            "loss -43377.91785019617\n",
            "iter  83\n",
            "loss -43169.05154762167\n",
            "iter  84\n",
            "loss -42964.45610101151\n",
            "iter  85\n",
            "loss -42763.99133241783\n",
            "iter  86\n",
            "loss -42567.523519407725\n",
            "iter  87\n",
            "loss -42374.925001309915\n",
            "iter  88\n",
            "loss -42186.07381554274\n",
            "iter  89\n",
            "loss -42000.853361485075\n",
            "iter  90\n",
            "loss -41819.152089544164\n",
            "iter  91\n",
            "loss -41640.863213245226\n",
            "iter  92\n",
            "loss -41465.88444239172\n",
            "iter  93\n",
            "loss -41294.11773546885\n",
            "iter  94\n",
            "loss -41125.46906966924\n",
            "iter  95\n",
            "loss -40959.848227008646\n",
            "iter  96\n",
            "loss -40797.16859519665\n",
            "iter  97\n",
            "loss -40637.34698198201\n",
            "iter  98\n",
            "loss -40480.30344185173\n",
            "iter  99\n",
            "loss -40325.96111404755\n",
            "iter  100\n",
            "loss -40174.24607095924\n",
            "iter  101\n",
            "loss -40025.087176009554\n",
            "iter  102\n",
            "loss -39878.41595028957\n",
            "iter  103\n",
            "loss -39734.16644719274\n",
            "iter  104\n",
            "loss -39592.27513443692\n",
            "iter  105\n",
            "loss -39452.68078286269\n",
            "iter  106\n",
            "loss -39315.324361496365\n",
            "iter  107\n",
            "loss -39180.148938387145\n",
            "iter  108\n",
            "loss -39047.09958679648\n",
            "iter  109\n",
            "loss -38916.12329635163\n",
            "iter  110\n",
            "loss -38787.16888880285\n",
            "iter  111\n",
            "loss -38660.18693808979\n",
            "iter  112\n",
            "loss -38535.129694424715\n",
            "iter  113\n",
            "loss -38411.95101214636\n",
            "iter  114\n",
            "loss -38290.60628111645\n",
            "iter  115\n",
            "loss -38171.05236146365\n",
            "iter  116\n",
            "loss -38053.24752148919\n",
            "iter  117\n",
            "loss -37937.15137856488\n",
            "iter  118\n",
            "loss -37822.724842885844\n",
            "iter  119\n",
            "loss -37709.93006393104\n",
            "iter  120\n",
            "loss -37598.73037950743\n",
            "iter  121\n",
            "loss -37489.09026726095\n",
            "iter  122\n",
            "loss -37380.97529854502\n",
            "iter  123\n",
            "loss -37274.352094528964\n",
            "iter  124\n",
            "loss -37169.18828446205\n",
            "iter  125\n",
            "loss -37065.45246597842\n",
            "iter  126\n",
            "loss -36963.11416735414\n",
            "iter  127\n",
            "loss -36862.14381162299\n",
            "iter  128\n",
            "loss -36762.51268245547\n",
            "iter  129\n",
            "loss -36664.19289171131\n",
            "iter  130\n",
            "loss -36567.15734857668\n",
            "iter  131\n",
            "loss -36471.37973019897\n",
            "iter  132\n",
            "loss -36376.834453737225\n",
            "iter  133\n",
            "loss -36283.49664973521\n",
            "iter  134\n",
            "loss -36191.34213675187\n",
            "iter  135\n",
            "loss -36100.34739715555\n",
            "iter  136\n",
            "loss -36010.48955402435\n",
            "iter  137\n",
            "loss -35921.74634906815\n",
            "iter  138\n",
            "loss -35834.0961215124\n",
            "iter  139\n",
            "loss -35747.51778788096\n",
            "iter  140\n",
            "loss -35661.99082261602\n",
            "iter  141\n",
            "loss -35577.495239485215\n",
            "iter  142\n",
            "loss -35494.0115737199\n",
            "iter  143\n",
            "loss -35411.520864842576\n",
            "iter  144\n",
            "loss -35330.00464013912\n",
            "iter  145\n",
            "loss -35249.44489874598\n",
            "iter  146\n",
            "loss -35169.82409629384\n",
            "iter  147\n",
            "loss -35091.12513012195\n",
            "iter  148\n",
            "loss -35013.33132497095\n",
            "iter  149\n",
            "loss -34936.42641921768\n",
            "iter  150\n",
            "loss -34860.39455150947\n",
            "iter  151\n",
            "loss -34785.2202479507\n",
            "iter  152\n",
            "loss -34710.888409582294\n",
            "iter  153\n",
            "loss -34637.38430048146\n",
            "iter  154\n",
            "loss -34564.69353598112\n",
            "iter  155\n",
            "loss -34492.80207170783\n",
            "iter  156\n",
            "loss -34421.69619237627\n",
            "iter  157\n",
            "loss -34351.36250190651\n",
            "iter  158\n",
            "loss -34281.78791250391\n",
            "iter  159\n",
            "loss -34212.95963625383\n",
            "iter  160\n",
            "loss -34144.86517391957\n",
            "iter  161\n",
            "loss -34077.49230916006\n",
            "iter  162\n",
            "loss -34010.82909615407\n",
            "iter  163\n",
            "loss -33944.86386022318\n",
            "iter  164\n",
            "loss -33879.585184939126\n",
            "iter  165\n",
            "loss -33814.98193568248\n",
            "iter  166\n",
            "loss -33751.04326779256\n",
            "iter  167\n",
            "loss -33687.75876757489\n",
            "iter  168\n",
            "loss -33625.118683048124\n",
            "iter  169\n",
            "loss -33563.114824739205\n",
            "iter  170\n",
            "loss -33501.74265879219\n",
            "iter  171\n",
            "loss -33441.00774100109\n",
            "iter  172\n",
            "loss -33380.94277228393\n",
            "iter  173\n",
            "loss -33321.65676215837\n",
            "iter  174\n",
            "loss -33263.47324255067\n",
            "iter  175\n",
            "loss -33207.32014251849\n",
            "iter  176\n",
            "loss -33155.87322957018\n",
            "iter  177\n",
            "loss -33116.689994707\n",
            "iter  178\n",
            "loss -33111.90503085254\n",
            "iter  179\n",
            "loss -33202.01698309022\n",
            "iter  180\n",
            "loss -33564.23634962459\n",
            "iter  181\n",
            "loss -34579.40132476136\n",
            "iter  182\n",
            "loss -37132.94122291729\n",
            "iter  183\n",
            "loss -40141.13352547963\n",
            "iter  184\n",
            "loss -44786.08391553217\n",
            "iter  185\n",
            "loss -38897.27039311063\n",
            "iter  186\n",
            "loss -40160.70287580817\n",
            "iter  187\n",
            "loss -37053.08876481592\n",
            "iter  188\n",
            "loss -37441.43339478945\n",
            "iter  189\n",
            "loss -36078.94911444578\n",
            "iter  190\n",
            "loss -36084.30143334051\n",
            "iter  191\n",
            "loss -35300.04935716572\n",
            "iter  192\n",
            "loss -35181.973323462225\n",
            "iter  193\n",
            "loss -34656.55335489919\n",
            "iter  194\n",
            "loss -34528.09047573654\n",
            "iter  195\n",
            "loss -34154.388503899565\n",
            "iter  196\n",
            "loss -34055.353571708045\n",
            "iter  197\n",
            "loss -33783.62110334538\n",
            "iter  198\n",
            "loss -33724.144080674705\n",
            "iter  199\n",
            "loss -33522.53092123233\n",
            "iter  200\n",
            "loss -33502.34713612508\n",
            "iter  201\n",
            "loss -33347.77800626795\n",
            "iter  202\n",
            "loss -33364.89033600664\n",
            "iter  203\n",
            "loss -33240.2359676297\n",
            "iter  204\n",
            "loss -33293.92283415293\n",
            "iter  205\n",
            "loss -33187.47342552435\n",
            "iter  206\n",
            "loss -33279.086839252595\n",
            "iter  207\n",
            "loss -33190.003754394136\n",
            "iter  208\n",
            "loss -33323.392028081464\n",
            "iter  209\n",
            "loss -33283.89308816066\n",
            "iter  210\n",
            "loss -33465.70295126372\n",
            "iter  211\n",
            "loss -33600.8041712027\n",
            "iter  212\n",
            "loss -33837.53446874736\n",
            "iter  213\n",
            "loss -34438.38928222487\n",
            "iter  214\n",
            "loss -34696.94421609234\n",
            "iter  215\n",
            "loss -35827.69123475572\n",
            "iter  216\n",
            "loss -35883.369549979536\n",
            "iter  217\n",
            "loss -36062.69788147369\n",
            "iter  218\n",
            "loss -35885.81764458949\n",
            "iter  219\n",
            "loss -34725.34000305507\n",
            "iter  220\n",
            "loss -34845.77689663147\n",
            "iter  221\n",
            "loss -33736.20709557822\n",
            "iter  222\n",
            "loss -34188.661761067924\n",
            "iter  223\n",
            "loss -33213.57490151508\n",
            "iter  224\n",
            "loss -33844.86856788092\n",
            "iter  225\n",
            "loss -32972.18637771065\n",
            "iter  226\n",
            "loss -33688.04984006831\n",
            "iter  227\n",
            "loss -32900.707772606846\n",
            "iter  228\n",
            "loss -33614.05137022747\n",
            "iter  229\n",
            "loss -32913.04758416847\n",
            "iter  230\n",
            "loss -33529.13338587322\n",
            "iter  231\n",
            "loss -32926.37630683415\n",
            "iter  232\n",
            "loss -33370.96457397293\n",
            "iter  233\n",
            "loss -32874.03226993117\n",
            "iter  234\n",
            "loss -33127.27015513561\n",
            "iter  235\n",
            "loss -32729.6987736495\n",
            "iter  236\n",
            "loss -32825.605100194785\n",
            "iter  237\n",
            "loss -32510.996880637875\n",
            "iter  238\n",
            "loss -32506.579252634445\n",
            "iter  239\n",
            "loss -32257.9061880408\n",
            "iter  240\n",
            "loss -32204.109692656344\n",
            "iter  241\n",
            "loss -32008.114900802444\n",
            "iter  242\n",
            "loss -31938.447420901535\n",
            "iter  243\n",
            "loss -31785.259136620836\n",
            "iter  244\n",
            "loss -31717.2637984993\n",
            "iter  245\n",
            "loss -31599.165877423242\n",
            "iter  246\n",
            "loss -31539.945417274088\n",
            "iter  247\n",
            "loss -31450.901148236717\n",
            "iter  248\n",
            "loss -31401.88756409047\n",
            "iter  249\n",
            "loss -31337.550632873772\n",
            "iter  250\n",
            "loss -31297.338007156428\n",
            "iter  251\n",
            "loss -31255.168864237123\n",
            "iter  252\n",
            "loss -31220.817428900977\n",
            "iter  253\n",
            "loss -31200.279628397115\n",
            "iter  254\n",
            "loss -31167.883505515107\n",
            "iter  255\n",
            "loss -31170.89489499139\n",
            "iter  256\n",
            "loss -31136.361484613924\n",
            "iter  257\n",
            "loss -31168.211278737443\n",
            "iter  258\n",
            "loss -31129.84609714391\n",
            "iter  259\n",
            "loss -31200.684376037578\n",
            "iter  260\n",
            "loss -31167.188184507544\n",
            "iter  261\n",
            "loss -31293.659540430144\n",
            "iter  262\n",
            "loss -31305.76333794123\n",
            "iter  263\n",
            "loss -31509.621885025597\n",
            "iter  264\n",
            "loss -31686.557622508877\n",
            "iter  265\n",
            "loss -31972.812082911823\n",
            "iter  266\n",
            "loss -32541.08762796857\n",
            "iter  267\n",
            "loss -32790.79405563009\n",
            "iter  268\n",
            "loss -33769.818602044776\n",
            "iter  269\n",
            "loss -33600.762396069316\n",
            "iter  270\n",
            "loss -34112.953473421774\n",
            "iter  271\n",
            "loss -33514.73662572905\n",
            "iter  272\n",
            "loss -33101.648144638864\n",
            "iter  273\n",
            "loss -32694.21373294853\n",
            "iter  274\n",
            "loss -32065.590957277072\n",
            "iter  275\n",
            "loss -31979.124825375977\n",
            "iter  276\n",
            "loss -31404.945690730125\n",
            "iter  277\n",
            "loss -31524.811309681987\n",
            "iter  278\n",
            "loss -31019.838561876273\n",
            "iter  279\n",
            "loss -31271.248250466713\n",
            "iter  280\n",
            "loss -30812.14803095953\n",
            "iter  281\n",
            "loss -31162.031426542508\n",
            "iter  282\n",
            "loss -30723.865734641207\n",
            "iter  283\n",
            "loss -31152.462189164293\n",
            "iter  284\n",
            "loss -30716.988583326103\n",
            "iter  285\n",
            "loss -31197.447923484906\n",
            "iter  286\n",
            "loss -30757.380438928463\n",
            "iter  287\n",
            "loss -31245.863666967223\n",
            "iter  288\n",
            "loss -30807.690284379387\n",
            "iter  289\n",
            "loss -31249.14552223988\n",
            "iter  290\n",
            "loss -30830.548070616805\n",
            "iter  291\n",
            "loss -31178.448904100776\n",
            "iter  292\n",
            "loss -30799.058728507734\n",
            "iter  293\n",
            "loss -31033.724171623493\n",
            "iter  294\n",
            "loss -30705.937291098522\n",
            "iter  295\n",
            "loss -30836.680188102167\n",
            "iter  296\n",
            "loss -30563.26408702905\n",
            "iter  297\n",
            "loss -30616.211207085496\n",
            "iter  298\n",
            "loss -30393.465553754257\n",
            "iter  299\n",
            "loss -30397.25889643251\n",
            "{'W1': array([[ 0.36166518, -0.17443647,  0.15642184, ..., -0.06199222,\n",
            "         0.31812079,  0.38571404],\n",
            "       [-0.31044838, -0.29199384,  0.01282363, ..., -0.16535607,\n",
            "         0.2462834 , -0.43044113],\n",
            "       [-0.35770905,  0.40621468,  0.00451352, ..., -0.22316441,\n",
            "         0.17451861, -0.35392002],\n",
            "       ...,\n",
            "       [ 0.01561846,  0.16271805, -0.36964025, ..., -0.36127005,\n",
            "        -0.20249197,  0.16128411],\n",
            "       [-0.01531504, -0.06003308, -0.02967414, ...,  0.25578234,\n",
            "        -0.18021031,  0.22204957],\n",
            "       [ 0.48130611,  0.34886796,  0.4860219 , ..., -0.29737718,\n",
            "         0.32871419, -0.34615788]]), 'b1': array([[-0.01424704],\n",
            "       [-0.30850687],\n",
            "       [-0.16080245],\n",
            "       [ 0.19472184],\n",
            "       [-0.3269283 ],\n",
            "       [-0.38668151],\n",
            "       [ 0.12400643],\n",
            "       [ 0.35974046],\n",
            "       [ 0.24455318],\n",
            "       [ 0.17888638],\n",
            "       [-0.36504475],\n",
            "       [ 0.00325823],\n",
            "       [ 0.0957651 ],\n",
            "       [-0.3832618 ],\n",
            "       [-0.46029171],\n",
            "       [-0.25798234],\n",
            "       [-0.43254374],\n",
            "       [ 0.16054648],\n",
            "       [ 0.52276781],\n",
            "       [ 0.03097587],\n",
            "       [-0.47713748],\n",
            "       [-0.48424284],\n",
            "       [-0.466037  ],\n",
            "       [ 0.48471569],\n",
            "       [-0.24903283],\n",
            "       [-0.10947057],\n",
            "       [ 0.46523163],\n",
            "       [-0.10071487],\n",
            "       [ 0.16922766],\n",
            "       [-0.27885297],\n",
            "       [ 0.0688202 ],\n",
            "       [-0.08880683],\n",
            "       [ 0.27006774],\n",
            "       [ 0.27935676],\n",
            "       [-0.03139327],\n",
            "       [-0.49954312],\n",
            "       [ 0.23171213],\n",
            "       [-0.48257818],\n",
            "       [-0.38146635],\n",
            "       [ 0.45018168],\n",
            "       [-0.11221576],\n",
            "       [-0.1600227 ],\n",
            "       [ 0.24236998],\n",
            "       [ 0.44666912],\n",
            "       [ 0.29790501],\n",
            "       [-0.35144519],\n",
            "       [-0.08834996],\n",
            "       [ 0.39966441],\n",
            "       [ 0.33365418],\n",
            "       [-0.09130433],\n",
            "       [-0.28715199],\n",
            "       [ 0.36264046],\n",
            "       [-0.04789322],\n",
            "       [-0.46810697],\n",
            "       [-0.02832055],\n",
            "       [-0.23041669],\n",
            "       [ 0.03809558],\n",
            "       [-0.14230818],\n",
            "       [ 0.48843423],\n",
            "       [-0.47272278],\n",
            "       [-0.32769559],\n",
            "       [ 0.12888087],\n",
            "       [-0.24815444],\n",
            "       [-0.40641445],\n",
            "       [ 0.00663999],\n",
            "       [-0.3551327 ],\n",
            "       [ 0.52633069],\n",
            "       [-0.21425227],\n",
            "       [ 0.11647419],\n",
            "       [-0.12393363],\n",
            "       [ 0.37123025],\n",
            "       [-0.48543654],\n",
            "       [-0.09153453],\n",
            "       [ 0.4095577 ],\n",
            "       [-0.07856784],\n",
            "       [ 0.52521471],\n",
            "       [ 0.21988172],\n",
            "       [ 0.21116814],\n",
            "       [ 0.33267965],\n",
            "       [ 0.14018219],\n",
            "       [-0.12073525],\n",
            "       [-0.40129895],\n",
            "       [ 0.49856583],\n",
            "       [-0.23029509],\n",
            "       [-0.02872527],\n",
            "       [ 0.20141915],\n",
            "       [ 0.02380296],\n",
            "       [-0.08599719],\n",
            "       [-0.54315748],\n",
            "       [ 0.21490974],\n",
            "       [ 0.02515844],\n",
            "       [-0.0166607 ],\n",
            "       [ 0.42581611],\n",
            "       [ 0.02169061],\n",
            "       [ 0.19165834],\n",
            "       [-0.02824111],\n",
            "       [ 0.10091409],\n",
            "       [-0.20197202],\n",
            "       [ 0.07463658],\n",
            "       [-0.35623969],\n",
            "       [ 0.13244571],\n",
            "       [ 0.14290337],\n",
            "       [-0.47172053],\n",
            "       [ 0.44429996],\n",
            "       [ 0.23643052],\n",
            "       [ 0.06281062],\n",
            "       [ 0.21312003],\n",
            "       [ 0.08918672],\n",
            "       [ 0.31090337],\n",
            "       [-0.16177912],\n",
            "       [-0.1237816 ],\n",
            "       [ 0.09323662],\n",
            "       [ 0.40261874],\n",
            "       [-0.12218426],\n",
            "       [ 0.11098733],\n",
            "       [-0.23232401],\n",
            "       [-0.44304025],\n",
            "       [ 0.20742809],\n",
            "       [ 0.12488157],\n",
            "       [-0.00309992]]), 'W2': array([[-0.07297424,  0.28368681, -0.0787073 , ..., -0.25981886,\n",
            "        -0.47285076,  0.27076   ],\n",
            "       [ 0.18415102, -0.1475142 ,  0.43369636, ...,  0.11655805,\n",
            "        -0.02499896, -0.51008381],\n",
            "       [-0.48024432,  0.07199414, -0.30524678, ...,  0.48769397,\n",
            "        -0.16379306, -0.45569092],\n",
            "       ...,\n",
            "       [-0.61154019, -0.36566637,  0.25468406, ...,  0.3618386 ,\n",
            "        -0.09549811,  0.1671592 ],\n",
            "       [ 0.44994062,  0.20384875,  0.28748137, ...,  0.3273833 ,\n",
            "        -0.04330039, -0.35750218],\n",
            "       [-0.35186694,  0.11101263, -0.01936593, ..., -0.7188593 ,\n",
            "        -0.29915748, -0.6515465 ]]), 'b2': array([[-0.13544435],\n",
            "       [-0.31840208],\n",
            "       [-0.31921033],\n",
            "       [ 0.46966951],\n",
            "       [ 0.015001  ],\n",
            "       [ 0.23183427],\n",
            "       [ 0.34613722],\n",
            "       [ 0.11788964],\n",
            "       [ 0.0435598 ],\n",
            "       [-0.44930184],\n",
            "       [-0.30670949],\n",
            "       [ 0.38375957],\n",
            "       [-0.06744209],\n",
            "       [ 0.30529146],\n",
            "       [-0.01087923],\n",
            "       [-0.21238876],\n",
            "       [ 0.05252094],\n",
            "       [-0.09118235],\n",
            "       [-0.12169077],\n",
            "       [ 0.53231135]]), 'W3': array([[ 1.13164969, -0.25354762,  0.23790274, -1.27205212,  0.89230253,\n",
            "         0.04602678,  0.45018559,  0.37685835, -0.791028  ,  0.11771489,\n",
            "        -0.35685684,  0.86702299,  0.65835705,  1.17034698,  0.13510139,\n",
            "        -0.88332232, -1.71115804,  0.89779976, -0.85155045,  0.20035584],\n",
            "       [ 0.17929765,  0.35362295,  1.18285319, -0.88840205,  0.21968538,\n",
            "         0.71805024,  1.28724727, -1.50900603,  1.42957191,  1.93954642,\n",
            "         0.00323732, -1.31070744, -0.47197671, -0.22556079,  0.39333249,\n",
            "        -1.30475043,  0.23207203, -0.62319231,  0.65213722, -0.90734469],\n",
            "       [ 0.97863793, -0.70051494, -0.53659194,  1.6610109 , -0.50617844,\n",
            "        -0.61917036, -0.34257976,  0.62177541, -1.24164636,  0.4671651 ,\n",
            "         0.60909822, -1.28224478, -0.70338854, -0.42756287, -0.0308212 ,\n",
            "         1.11622282,  0.34124105,  1.02592843, -0.94766009,  1.04337393],\n",
            "       [ 0.39538841, -0.21049391, -0.45554027, -0.90046638,  0.43178914,\n",
            "        -0.91814483,  1.51768888,  0.54116186, -1.40122461,  0.60819344,\n",
            "         0.2034307 ,  0.50523361,  0.41870911, -0.72691453, -0.94106111,\n",
            "        -1.71263627,  1.81463383, -0.86593903,  0.27520685,  0.86060408],\n",
            "       [ 0.67063341, -0.72159121, -0.5077576 ,  0.93299535, -0.85467505,\n",
            "         0.51751679,  0.65017114, -1.52012611, -0.13746162,  0.98540953,\n",
            "         0.5109769 , -0.07945333, -1.42670036,  0.55343963, -0.81111679,\n",
            "         0.83321826,  0.49022227, -1.07789838, -0.56717706,  1.06298524],\n",
            "       [-1.3081271 ,  0.91038709,  0.69723524,  1.189377  ,  0.30745494,\n",
            "        -1.45069116, -1.15333695,  0.01267132,  1.26303814, -0.53204801,\n",
            "         1.07984895, -0.02386348,  0.03231247,  0.43996852, -0.72303447,\n",
            "         0.40863708, -0.07857032,  1.70892311,  0.88016824, -1.90341269],\n",
            "       [ 0.9747449 , -0.1066319 , -0.31890695,  0.45762604,  0.94081199,\n",
            "         0.96457474, -0.10099809, -0.07365247, -1.12074723, -1.19841198,\n",
            "         0.07189516,  0.87648436, -1.09345805,  0.43894838, -0.06920667,\n",
            "         0.80106669, -0.33587466, -0.56144606, -1.06284787,  0.68717224],\n",
            "       [-1.48456253, -0.55293505, -1.15551514, -1.01180336,  0.54308765,\n",
            "         0.20775501, -1.15542341,  0.72519008,  1.4410089 , -0.86639585,\n",
            "        -1.55064725, -1.32532946,  1.24616896,  0.52533815, -0.48571721,\n",
            "         0.23267237,  1.31523261,  0.47275322,  0.90740626, -1.51456294],\n",
            "       [-1.59882527,  1.09201483, -1.09055592,  0.94376502, -0.72423114,\n",
            "        -0.31466409, -0.72831086,  0.25498552,  0.42792831, -0.31148846,\n",
            "         1.19537401,  0.53186506,  0.42276255,  0.08829499, -1.0227928 ,\n",
            "         0.00972977, -1.79693204, -1.64611854,  1.36520469,  1.46225805],\n",
            "       [ 0.29500145,  1.00525201,  0.63119921, -1.35456425, -1.57169703,\n",
            "        -0.83751237, -0.68965944,  0.12938593,  1.23206026, -0.95509907,\n",
            "        -1.53974074, -0.76098294,  0.48030642, -1.44507007,  1.87595177,\n",
            "         1.33049779,  0.02194988,  0.18432255,  1.13994368, -0.27171448]]), 'b3': array([[-0.04983381],\n",
            "       [ 0.15845401],\n",
            "       [ 0.03483076],\n",
            "       [ 0.33369436],\n",
            "       [ 0.03156253],\n",
            "       [-0.48538444],\n",
            "       [ 0.52116445],\n",
            "       [ 0.46162495],\n",
            "       [ 0.38282376],\n",
            "       [ 0.33198546]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpuXKyTzYJnB",
        "outputId": "b3041b66-7bbb-4a16-c480-f8e260fa97f8"
      },
      "source": [
        "def predict(X_test,y_test):\n",
        "  _,_,y_hat=forwardPropagation(X_test,parameters)\n",
        "  y_hat=y_hat.argmax(axis=0)\n",
        "  correctPred=np.sum(y_hat==y_test)\n",
        "  print(\"Accuracy is \", correctPred/X_test.shape[0]*100,\"%\")\n",
        "  return (y_hat,correctPred)\n",
        "print(predict(X_train,y_train))\n",
        "print(predict(X_test,y_test))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  82.0 %\n",
            "(array([9, 0, 3, ..., 3, 0, 5]), 49200)\n",
            "Accuracy is  80.58999999999999 %\n",
            "(array([9, 2, 1, ..., 8, 1, 5]), 8059)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}