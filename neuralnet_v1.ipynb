{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assgnment1_DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVjuXsr1zLu2"
      },
      "source": [
        "#import the dataset \n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQLWjHat0lV4"
      },
      "source": [
        "#import the required libraries \n",
        "from matplotlib import pyplot\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP17UY7v0C7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4319d507-9247-4bfb-fba5-9562be38964d"
      },
      "source": [
        "#load the data into train and test \n",
        "(X_train,y_train),(X_test,y_test)=fashion_mnist.load_data()\n",
        "#pyplot.imshow(X_train[2])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z27sdlQmMH1O",
        "outputId": "a18b6770-e0aa-4da1-90ac-fef9da63bfec"
      },
      "source": [
        "noOfImages = X_train.shape[0]\n",
        "X_train = (1.0/255)*np.array([X_train[i].flatten() for i in range(0,noOfImages)])\n",
        "X_train = np.array([X_train[i].flatten() for i in range(0,noOfImages)])\n",
        "noOftestImages= X_test.shape[0]\n",
        "X_test =(1.0/255)*np.array([X_test[i].flatten() for i in range(0,noOftestImages)])\n",
        "X_test =np.array([X_test[i].flatten() for i in range(0,noOftestImages)])\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtQEmgi3Opy_",
        "outputId": "c7df542a-3e68-4206-c9d5-ead69b5b101a"
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
            " 0.         0.00392157 0.01568627 0.         0.         0.\n",
            " 0.         0.00392157 0.00392157 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.01176471 0.\n",
            " 0.14117647 0.53333333 0.49803922 0.24313725 0.21176471 0.\n",
            " 0.         0.         0.00392157 0.01176471 0.01568627 0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.02352941 0.         0.4        0.8\n",
            " 0.69019608 0.5254902  0.56470588 0.48235294 0.09019608 0.\n",
            " 0.         0.         0.         0.04705882 0.03921569 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
            " 0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
            " 0.30196078 0.50980392 0.28235294 0.05882353 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00392157 0.         0.27058824\n",
            " 0.81176471 0.8745098  0.85490196 0.84705882 0.84705882 0.63921569\n",
            " 0.49803922 0.4745098  0.47843137 0.57254902 0.55294118 0.34509804\n",
            " 0.6745098  0.25882353 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.00392157 0.00392157 0.         0.78431373 0.90980392 0.90980392\n",
            " 0.91372549 0.89803922 0.8745098  0.8745098  0.84313725 0.83529412\n",
            " 0.64313725 0.49803922 0.48235294 0.76862745 0.89803922 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
            " 0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
            " 0.8745098  0.96078431 0.67843137 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.75686275\n",
            " 0.89411765 0.85490196 0.83529412 0.77647059 0.70588235 0.83137255\n",
            " 0.82352941 0.82745098 0.83529412 0.8745098  0.8627451  0.95294118\n",
            " 0.79215686 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.01176471 0.         0.04705882 0.85882353 0.8627451  0.83137255\n",
            " 0.85490196 0.75294118 0.6627451  0.89019608 0.81568627 0.85490196\n",
            " 0.87843137 0.83137255 0.88627451 0.77254902 0.81960784 0.20392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.02352941 0.\n",
            " 0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
            " 0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
            " 0.96078431 0.46666667 0.65490196 0.21960784 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.01568627 0.         0.         0.21568627 0.9254902\n",
            " 0.89411765 0.90196078 0.89411765 0.94117647 0.90980392 0.83529412\n",
            " 0.85490196 0.8745098  0.91764706 0.85098039 0.85098039 0.81960784\n",
            " 0.36078431 0.         0.         0.         0.00392157 0.01568627\n",
            " 0.02352941 0.02745098 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.92941176 0.88627451 0.85098039 0.8745098\n",
            " 0.87058824 0.85882353 0.87058824 0.86666667 0.84705882 0.8745098\n",
            " 0.89803922 0.84313725 0.85490196 1.         0.30196078 0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.24313725 0.56862745 0.8\n",
            " 0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
            " 0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
            " 0.87843137 0.95686275 0.62352941 0.         0.         0.\n",
            " 0.         0.         0.07058824 0.17254902 0.32156863 0.41960784\n",
            " 0.74117647 0.89411765 0.8627451  0.87058824 0.85098039 0.88627451\n",
            " 0.78431373 0.80392157 0.82745098 0.90196078 0.87843137 0.91764706\n",
            " 0.69019608 0.7372549  0.98039216 0.97254902 0.91372549 0.93333333\n",
            " 0.84313725 0.         0.         0.22352941 0.73333333 0.81568627\n",
            " 0.87843137 0.86666667 0.87843137 0.81568627 0.8        0.83921569\n",
            " 0.81568627 0.81960784 0.78431373 0.62352941 0.96078431 0.75686275\n",
            " 0.80784314 0.8745098  1.         1.         0.86666667 0.91764706\n",
            " 0.86666667 0.82745098 0.8627451  0.90980392 0.96470588 0.\n",
            " 0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
            " 0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
            " 0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
            " 0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
            " 0.87058824 0.89411765 0.88235294 0.         0.38431373 0.91372549\n",
            " 0.77647059 0.82352941 0.87058824 0.89803922 0.89803922 0.91764706\n",
            " 0.97647059 0.8627451  0.76078431 0.84313725 0.85098039 0.94509804\n",
            " 0.25490196 0.28627451 0.41568627 0.45882353 0.65882353 0.85882353\n",
            " 0.86666667 0.84313725 0.85098039 0.8745098  0.8745098  0.87843137\n",
            " 0.89803922 0.11372549 0.29411765 0.8        0.83137255 0.8\n",
            " 0.75686275 0.80392157 0.82745098 0.88235294 0.84705882 0.7254902\n",
            " 0.77254902 0.80784314 0.77647059 0.83529412 0.94117647 0.76470588\n",
            " 0.89019608 0.96078431 0.9372549  0.8745098  0.85490196 0.83137255\n",
            " 0.81960784 0.87058824 0.8627451  0.86666667 0.90196078 0.2627451\n",
            " 0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
            " 0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
            " 0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
            " 0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
            " 0.70980392 0.80392157 0.80784314 0.45098039 0.         0.47843137\n",
            " 0.85882353 0.75686275 0.70196078 0.67058824 0.71764706 0.76862745\n",
            " 0.8        0.82352941 0.83529412 0.81176471 0.82745098 0.82352941\n",
            " 0.78431373 0.76862745 0.76078431 0.74901961 0.76470588 0.74901961\n",
            " 0.77647059 0.75294118 0.69019608 0.61176471 0.65490196 0.69411765\n",
            " 0.82352941 0.36078431 0.         0.         0.29019608 0.74117647\n",
            " 0.83137255 0.74901961 0.68627451 0.6745098  0.68627451 0.70980392\n",
            " 0.7254902  0.7372549  0.74117647 0.7372549  0.75686275 0.77647059\n",
            " 0.8        0.81960784 0.82352941 0.82352941 0.82745098 0.7372549\n",
            " 0.7372549  0.76078431 0.75294118 0.84705882 0.66666667 0.\n",
            " 0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
            " 0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
            " 0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
            " 0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
            " 0.38823529 0.22745098 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.15686275\n",
            " 0.23921569 0.17254902 0.28235294 0.16078431 0.1372549  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFA10HAHKCb8"
      },
      "source": [
        "def softmax(X):\n",
        "  X=np.exp(X)\n",
        "  sum=np.sum(X,axis=0)\n",
        "  return X/sum \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZgtMpEAUkIA"
      },
      "source": [
        "def sigmoidFunc(X):\n",
        "\n",
        "  return 1.0/(1.0+np.exp(-X))\n",
        "  #return res"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GJHo_F55wKD"
      },
      "source": [
        "def gDash(X):\n",
        "  return sigmoidFunc(X)*(1-sigmoidFunc(X))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIfFadrLAkmM"
      },
      "source": [
        "def forwardPropagation(X,parameters):\n",
        "  preactivation={}\n",
        "  activation={}\n",
        "  activation['h0']=X.T\n",
        " \n",
        "  for k in range(1,noOfHiddenLayers+1):\n",
        "    preactivation['a'+str(k)]=np.dot(parameters['W'+str(k)],activation['h'+str(k-1)])+parameters['b'+str(k)]\n",
        "    activation['h'+str(k)]=sigmoidFunc(preactivation['a'+str(k)])\n",
        " \n",
        "  preactivation['a'+str(noOfHiddenLayers+1)]=np.dot(parameters['W'+str(noOfHiddenLayers+1)],activation['h'+str(noOfHiddenLayers)])+parameters['b'+str(noOfHiddenLayers+1)]\n",
        "  y=softmax(preactivation['a'+str(noOfHiddenLayers+1)])    \n",
        "\n",
        "  return (preactivation,activation,y)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZFsUg0xrJtv"
      },
      "source": [
        "def backPropagation(parameters,activation,preactivation,yhat,X,y_train):\n",
        "  grads={}\n",
        "  eIndicator=np.zeros((10,X.shape[0]))\n",
        "  eIndicator[y_train,np.arange(X.shape[0])]=1\n",
        "\n",
        "  grads['a'+str(noOfHiddenLayers+1)]= -(eIndicator-yhat)\n",
        "\n",
        "  for j in range(noOfHiddenLayers+1,0,-1):\n",
        "    grads['W'+str(j)]= np.dot(grads['a'+str(j)],activation['h'+str(j-1)].T)\n",
        " \n",
        "    grads['b'+str(j)]= np.sum(grads['a'+str(j)],axis=1,keepdims=True)\n",
        "    \n",
        "    grads['h'+str(j-1)]=np.dot(parameters['W'+str(j)].T,grads['a'+str(j)])\n",
        "    \n",
        "    if (j!=1):\n",
        "      grads['a'+str(j-1)]=grads['h'+str(j-1)]*gDash(preactivation['a'+str(j-1)])\n",
        "  return grads\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MXQ17Gk_Cuv"
      },
      "source": [
        "def Loss(yhat,y_train,X):\n",
        "  eIndicator=np.zeros((10,X.shape[0]))\n",
        "  eIndicator[y_train,np.arange(X.shape[0])]=1\n",
        "  eIndicator=eIndicator*yhat\n",
        "  eIndicator=eIndicator.sum(axis=0)\n",
        "  eIndicator=np.log(eIndicator)\n",
        "  return -sum(eIndicator)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnJHWPgMiiLw",
        "outputId": "2ce57c6e-8ef4-4a53-892a-5b0cb4a894de"
      },
      "source": [
        "\n",
        "l=10 #output classes\n",
        "noOfneuronsEach=[120,20]\n",
        "noOfHiddenLayers=len(noOfneuronsEach)\n",
        "inputNeuronSize=X_train.shape[1]\n",
        "print(inputNeuronSize)\n",
        "parameters={}\n",
        "batchSize=600\n",
        "iterations=100\n",
        "#eta=1   \n",
        "eta=0.001   #for rmsprop higher learning rate gradients explode causing overflow\n",
        "eta=0.002 #Nadam\n",
        "#input W\n",
        "#initialization\n",
        "parameters['W'+str(1)] =np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[0], inputNeuronSize))\n",
        "parameters['b'+str(1)]= np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[0],1))\n",
        "#parameters['W'+str(1)] =0+1.5*np.random.randn(noOfneuronsEach[0], inputNeuronSize)\n",
        "#parameters['b'+str(1)]= 0+1.5*np.random.randn(noOfneuronsEach[0],1)\n",
        "for i in range(2,noOfHiddenLayers+1):\n",
        "  parameters['W'+str(i)] = np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "  parameters['b'+str(i)]= np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[i-1],1))\n",
        "  #parameters['W'+str(i)] = 0+1.5*np.random.randn(noOfneuronsEach[i-1], noOfneuronsEach[i-2])\n",
        "  #parameters['b'+str(i)]= 0+1.5*np.random.randn(noOfneuronsEach[i-1],1)\n",
        "#Output W\n",
        "parameters['W'+str(noOfHiddenLayers+1)] = np.random.uniform(low=-0.5,high=0.5,size=(l, noOfneuronsEach[-1]))\n",
        "parameters['b'+str(noOfHiddenLayers+1)]= np.random.uniform(low=-0.5,high=0.5,size=(l,1))\n",
        "#parameters['W'+str(noOfHiddenLayers+1)] = 0+1.5*np.random.randn(l, noOfneuronsEach[-1])\n",
        "#parameters['b'+str(noOfHiddenLayers+1)]= 0+1.5*np.random.randn(l,1)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "3rIfc41ocC7-",
        "outputId": "7474480f-d8f8-4280-8410-8da51b818be4"
      },
      "source": [
        "def vanillagradDescent():\n",
        "  t=0\n",
        "  while(t < iterations):\n",
        "    print(\"Epoch \",t)\n",
        "    #print(parameters)\n",
        "    mini=0\n",
        "    while(mini<(noOfImages/batchSize)):\n",
        "      #print(\"Batch \",mini)\n",
        "      X_mini=X_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      y_mini=y_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      preactivation,activation,yhat=forwardPropagation(X_mini,parameters)\n",
        "      gradients=backPropagation(parameters,activation,preactivation,yhat,X_mini,y_mini)\n",
        "    #print(\"gradients\",gradients)\n",
        "      for i in range(1,noOfHiddenLayers+2):\n",
        "        parameters['W'+str(i)] -=  eta*(1.0/X_mini.shape[0])*gradients['W'+str(i)]\n",
        "        parameters['b'+str(i)] -= eta*(1.0/X_mini.shape[0])*gradients['b'+str(i)]\n",
        "      mini+=1\n",
        "    _,_,yhat=forwardPropagation(X_train,parameters)\n",
        "    print(\"loss after epoch \",Loss(yhat,y_train,X_train))\n",
        "    \n",
        "    t+=1\n",
        "vanillagradDescent()\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "loss after epoch  47573.393426219605\n",
            "Epoch  1\n",
            "loss after epoch  41904.39835775272\n",
            "Epoch  2\n",
            "loss after epoch  33197.24226676189\n",
            "Epoch  3\n",
            "loss after epoch  30161.114459625456\n",
            "Epoch  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-1fc27ecb55d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#print(yhat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mvanillagradDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(parameters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#print(gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-1fc27ecb55d6>\u001b[0m in \u001b[0;36mvanillagradDescent\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mX_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0my_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#print(\"gradients\",gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-52e34919ac23>\u001b[0m in \u001b[0;36mforwardPropagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(activation['h0'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoOfHiddenLayers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoidFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print('h size '+str(k),activation['h'+str(k)].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5PBJIPYtcLmg",
        "outputId": "f6120fe4-267f-480f-ff47-8eea00e4b550"
      },
      "source": [
        "def momentumGD():\n",
        "  #initialization\n",
        "  prevW={}\n",
        "  prevb={}\n",
        "  gamma=0.9\n",
        "  prevW['W'+str(1)] =np.zeros((noOfneuronsEach[0], inputNeuronSize))\n",
        "  prevb['b'+str(1)]= np.zeros((noOfneuronsEach[0],1))\n",
        "  for i in range(2,noOfHiddenLayers+1):\n",
        "    prevW['W'+str(i)] = np.zeros((noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "    prevb['b'+str(i)]= np.zeros((noOfneuronsEach[i-1],1))\n",
        "  prevW['W'+str(noOfHiddenLayers+1)] = np.zeros((l, noOfneuronsEach[-1]))\n",
        "  prevb['b'+str(noOfHiddenLayers+1)]= np.zeros((l,1))\n",
        "  t=0\n",
        "  while(t<iterations):\n",
        "    print(\"Epoch \",t)\n",
        "    mini=0\n",
        "    while(mini<(noOfImages/batchSize)):\n",
        "      #print(\"Batch \",mini)\n",
        "      X_mini=X_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      y_mini=y_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      preactivation,activation,yhat=forwardPropagation(X_mini,parameters)\n",
        "      gradients=backPropagation(parameters,activation,preactivation,yhat,X_mini,y_mini)\n",
        "      #print(\"gradients\",gradients)\n",
        "      for i in range(1,noOfHiddenLayers+2):\n",
        "        w=gamma*prevW['W'+str(i)]+eta*(1.0/X_mini.shape[0])*gradients['W'+str(i)]\n",
        "        b=gamma*prevb['b'+str(i)]+eta*(1.0/X_mini.shape[0])*gradients['b'+str(i)]\n",
        "        parameters['W'+str(i)] -=  w\n",
        "        parameters['b'+str(i)] -=  b\n",
        "        prevW['W'+str(i)]=w\n",
        "        prevb['b'+str(i)]=b\n",
        "      mini+=1\n",
        "    _,_,yhat=forwardPropagation(X_train,parameters)\n",
        "    print(\"loss after epoch \",Loss(yhat,y_train,X_train))\n",
        "    \n",
        "    t+=1\n",
        "momentumGD()\n",
        "\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "loss after epoch  139104.58233516416\n",
            "Epoch  1\n",
            "loss after epoch  135653.34137150904\n",
            "Epoch  2\n",
            "loss after epoch  133530.29032065018\n",
            "Epoch  3\n",
            "loss after epoch  131507.08960641167\n",
            "Epoch  4\n",
            "loss after epoch  129349.62693123208\n",
            "Epoch  5\n",
            "loss after epoch  127035.64020326082\n",
            "Epoch  6\n",
            "loss after epoch  124597.91580561991\n",
            "Epoch  7\n",
            "loss after epoch  122062.11478591064\n",
            "Epoch  8\n",
            "loss after epoch  119437.66889407003\n",
            "Epoch  9\n",
            "loss after epoch  116732.94189523862\n",
            "Epoch  10\n",
            "loss after epoch  113966.5784082077\n",
            "Epoch  11\n",
            "loss after epoch  111173.31822416405\n",
            "Epoch  12\n",
            "loss after epoch  108400.25914087553\n",
            "Epoch  13\n",
            "loss after epoch  105690.34601707169\n",
            "Epoch  14\n",
            "loss after epoch  103070.88501845609\n",
            "Epoch  15\n",
            "loss after epoch  100556.71147232091\n",
            "Epoch  16\n",
            "loss after epoch  98155.97612306397\n",
            "Epoch  17\n",
            "loss after epoch  95871.9689163332\n",
            "Epoch  18\n",
            "loss after epoch  93703.44196097103\n",
            "Epoch  19\n",
            "loss after epoch  91645.66990811274\n",
            "Epoch  20\n",
            "loss after epoch  89692.0732574155\n",
            "Epoch  21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-d0ca75688182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmomentumGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-d0ca75688182>\u001b[0m in \u001b[0;36mmomentumGD\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mX_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0my_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;31m#print(\"gradients\",gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-52e34919ac23>\u001b[0m in \u001b[0;36mforwardPropagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(activation['h0'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoOfHiddenLayers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoidFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print('h size '+str(k),activation['h'+str(k)].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "JKIw16fnauT7",
        "outputId": "9a818fb3-6184-49a6-cb5f-798bf0c968fc"
      },
      "source": [
        "def rmsProp():\n",
        "  #initialization\n",
        "  prevW={}\n",
        "  prevb={}\n",
        "  epsilon=1e-8\n",
        "  beta=0.9\n",
        "  prevW['W'+str(1)] =np.zeros((noOfneuronsEach[0], inputNeuronSize))\n",
        "  prevb['b'+str(1)]= np.zeros((noOfneuronsEach[0],1))\n",
        "  for i in range(2,noOfHiddenLayers+1):\n",
        "    prevW['W'+str(i)] = np.zeros((noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "    prevb['b'+str(i)]= np.zeros((noOfneuronsEach[i-1],1))\n",
        "  prevW['W'+str(noOfHiddenLayers+1)] = np.zeros((l, noOfneuronsEach[-1]))\n",
        "  prevb['b'+str(noOfHiddenLayers+1)]= np.zeros((l,1))\n",
        "  t=0\n",
        "  while(t<iterations):\n",
        "    print(\"Epoch \",t)\n",
        "    mini=0\n",
        "    while(mini<(noOfImages/batchSize)):\n",
        "      #print(\"Batch \",mini)\n",
        "      X_mini=X_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      y_mini=y_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      preactivation,activation,yhat=forwardPropagation(X_mini,parameters)\n",
        "      gradients=backPropagation(parameters,activation,preactivation,yhat,X_mini,y_mini)\n",
        "      #print(\"gradients\",gradients)\n",
        "      for i in range(1,noOfHiddenLayers+2):\n",
        "        prevW['W'+str(i)]= beta*prevW['W'+str(i)]+(1.0 - beta)* (gradients['W'+str(i)]**2)\n",
        "        prevb['b'+str(i)]= beta*prevb['b'+str(i)]+(1.0 - beta)* (gradients['b'+str(i)]**2)\n",
        "        \n",
        "        parameters['W'+str(i)] -=  ((eta/np.sqrt(prevW['W'+str(i)]+epsilon))*gradients['W'+str(i)])\n",
        "        parameters['b'+str(i)] -=  ((eta/np.sqrt(prevb['b'+str(i)]+epsilon))*gradients['b'+str(i)])\n",
        "      mini+=1\n",
        "    _,_,yhat=forwardPropagation(X_train,parameters)\n",
        "    print(\"loss after epoch \",Loss(yhat,y_train,X_train))\n",
        "    \n",
        "    t+=1\n",
        "rmsProp()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "loss after epoch  51233.51974791119\n",
            "Epoch  1\n",
            "loss after epoch  39667.038907740905\n",
            "Epoch  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-f048002c1f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrmsProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-f048002c1f63>\u001b[0m in \u001b[0;36mrmsProp\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0my_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0;31m#print(\"gradients\",gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoOfHiddenLayers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-ba42ea4acd27>\u001b[0m in \u001b[0;36mbackPropagation\u001b[0;34m(parameters, activation, preactivation, yhat, X, y_train)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#print('a'+str(j-1),preactivation['a'+str(j-1)].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgDash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#backPropagation()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-70b9c3967545>\u001b[0m in \u001b[0;36mgDash\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgDash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msigmoidFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigmoidFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnmSv72ylsBG",
        "outputId": "2a55e424-6e2b-4a51-fd3c-bdb25313c7fb"
      },
      "source": [
        "def nadam():\n",
        "  #initialization\n",
        "  prevW={}\n",
        "  prevb={}\n",
        "  mW={}\n",
        "  mb={}\n",
        "  epsilon=1e-8\n",
        "  beta1=0.9\n",
        "  beta2=0.999\n",
        "  prevW['W'+str(1)] =np.zeros((noOfneuronsEach[0], inputNeuronSize))\n",
        "  prevb['b'+str(1)]= np.zeros((noOfneuronsEach[0],1))\n",
        "  for i in range(2,noOfHiddenLayers+1):\n",
        "    prevW['W'+str(i)] = np.zeros((noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "    prevb['b'+str(i)]= np.zeros((noOfneuronsEach[i-1],1))\n",
        "  prevW['W'+str(noOfHiddenLayers+1)] = np.zeros((l, noOfneuronsEach[-1]))\n",
        "  prevb['b'+str(noOfHiddenLayers+1)]= np.zeros((l,1))\n",
        "\n",
        "  mW['W'+str(1)] =np.zeros((noOfneuronsEach[0], inputNeuronSize))\n",
        "  mb['b'+str(1)]= np.zeros((noOfneuronsEach[0],1))\n",
        "  for i in range(2,noOfHiddenLayers+1):\n",
        "    mW['W'+str(i)] = np.zeros((noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "    mb['b'+str(i)]= np.zeros((noOfneuronsEach[i-1],1))\n",
        "  mW['W'+str(noOfHiddenLayers+1)] = np.zeros((l, noOfneuronsEach[-1]))\n",
        "  mb['b'+str(noOfHiddenLayers+1)]= np.zeros((l,1))\n",
        "\n",
        "\n",
        "  t=0 #iterations \n",
        "  f=0 # update number \n",
        "  while(t<iterations):\n",
        "    print(\"Epoch \",t)\n",
        "    mini=0\n",
        "    while(mini<(noOfImages/batchSize)):\n",
        "      #print(\"Batch \",mini)\n",
        "      X_mini=X_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      y_mini=y_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      preactivation,activation,yhat=forwardPropagation(X_mini,parameters)\n",
        "      gradients=backPropagation(parameters,activation,preactivation,yhat,X_mini,y_mini)\n",
        "      f+=1\n",
        "      for i in range(1,noOfHiddenLayers+2):\n",
        "        mW['W'+str(i)]= beta1*mW['W'+str(i)]+(1.0 - beta1)* (gradients['W'+str(i)])\n",
        "        mb['b'+str(i)]= beta1*mb['b'+str(i)]+(1.0 - beta1)* (gradients['b'+str(i)])\n",
        "        \n",
        "        prevW['W'+str(i)]= beta2*prevW['W'+str(i)]+(1.0 - beta2)* (gradients['W'+str(i)]**2)\n",
        "        prevb['b'+str(i)]= beta2*prevb['b'+str(i)]+(1.0 - beta2)* (gradients['b'+str(i)]**2)\n",
        "\n",
        "        mWHat=(1.0-(beta1**f))*mW['W'+str(i)]\n",
        "        mbHat=(1.0-(beta1**f))*mb['b'+str(i)]\n",
        "\n",
        "        vWHat=(1.0-(beta2**f))*prevW['W'+str(i)]\n",
        "        vbHat=(1.0-(beta2**f))*prevb['b'+str(i)]\n",
        "        \n",
        "        mbarW= beta1*mWHat+(1.0-beta1)*gradients['W'+str(i)]\n",
        "        mbarb= beta1*mbHat+(1.0-beta1)*gradients['b'+str(i)]\n",
        "\n",
        "        parameters['W'+str(i)] -=  ((eta/np.sqrt(vWHat+epsilon))*mbarW)\n",
        "        parameters['b'+str(i)] -=  ((eta/np.sqrt(vbHat+epsilon))*mbarb)\n",
        "      mini+=1\n",
        "    _,_,yhat=forwardPropagation(X_train,parameters)\n",
        "    print(\"loss after epoch \",Loss(yhat,y_train,X_train))\n",
        "    \n",
        "    t+=1\n",
        "nadam()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "loss after epoch  65691.24430290182\n",
            "Epoch  1\n",
            "loss after epoch  56191.56502188217\n",
            "Epoch  2\n",
            "loss after epoch  44798.44039205763\n",
            "Epoch  3\n",
            "loss after epoch  40620.18673769921\n",
            "Epoch  4\n",
            "loss after epoch  38342.535301488264\n",
            "Epoch  5\n",
            "loss after epoch  36133.78848749312\n",
            "Epoch  6\n",
            "loss after epoch  33816.36261855071\n",
            "Epoch  7\n",
            "loss after epoch  32288.854075853724\n",
            "Epoch  8\n",
            "loss after epoch  31161.612311450663\n",
            "Epoch  9\n",
            "loss after epoch  30301.327255245636\n",
            "Epoch  10\n",
            "loss after epoch  29598.33189718381\n",
            "Epoch  11\n",
            "loss after epoch  29011.85155684418\n",
            "Epoch  12\n",
            "loss after epoch  28489.76537472226\n",
            "Epoch  13\n",
            "loss after epoch  27994.386162485807\n",
            "Epoch  14\n",
            "loss after epoch  27552.16427285564\n",
            "Epoch  15\n",
            "loss after epoch  27130.39653252381\n",
            "Epoch  16\n",
            "loss after epoch  26729.488187380473\n",
            "Epoch  17\n",
            "loss after epoch  26360.351182323997\n",
            "Epoch  18\n",
            "loss after epoch  26020.42627632413\n",
            "Epoch  19\n",
            "loss after epoch  25699.979826872903\n",
            "Epoch  20\n",
            "loss after epoch  25402.369181332084\n",
            "Epoch  21\n",
            "loss after epoch  25114.44558105239\n",
            "Epoch  22\n",
            "loss after epoch  24848.979254068163\n",
            "Epoch  23\n",
            "loss after epoch  24602.73031806707\n",
            "Epoch  24\n",
            "loss after epoch  24364.18608225348\n",
            "Epoch  25\n",
            "loss after epoch  24124.516052089366\n",
            "Epoch  26\n",
            "loss after epoch  23898.318540477256\n",
            "Epoch  27\n",
            "loss after epoch  23680.751897318904\n",
            "Epoch  28\n",
            "loss after epoch  23472.442300061193\n",
            "Epoch  29\n",
            "loss after epoch  23275.2203370392\n",
            "Epoch  30\n",
            "loss after epoch  23086.86210844271\n",
            "Epoch  31\n",
            "loss after epoch  22909.290289235287\n",
            "Epoch  32\n",
            "loss after epoch  22736.468258471657\n",
            "Epoch  33\n",
            "loss after epoch  22568.07895672337\n",
            "Epoch  34\n",
            "loss after epoch  22407.778049746732\n",
            "Epoch  35\n",
            "loss after epoch  22246.693171126837\n",
            "Epoch  36\n",
            "loss after epoch  22092.2306492822\n",
            "Epoch  37\n",
            "loss after epoch  21924.84460356081\n",
            "Epoch  38\n",
            "loss after epoch  21765.438854012118\n",
            "Epoch  39\n",
            "loss after epoch  21606.697705974482\n",
            "Epoch  40\n",
            "loss after epoch  21450.13360006984\n",
            "Epoch  41\n",
            "loss after epoch  21310.523524418513\n",
            "Epoch  42\n",
            "loss after epoch  21139.72499938234\n",
            "Epoch  43\n",
            "loss after epoch  20980.84133504062\n",
            "Epoch  44\n",
            "loss after epoch  20833.212198736754\n",
            "Epoch  45\n",
            "loss after epoch  20698.524542834723\n",
            "Epoch  46\n",
            "loss after epoch  20570.559901234272\n",
            "Epoch  47\n",
            "loss after epoch  20441.348914072394\n",
            "Epoch  48\n",
            "loss after epoch  20320.98452804362\n",
            "Epoch  49\n",
            "loss after epoch  20189.80733482999\n",
            "Epoch  50\n",
            "loss after epoch  20065.515126688348\n",
            "Epoch  51\n",
            "loss after epoch  19945.322561537283\n",
            "Epoch  52\n",
            "loss after epoch  19829.17066010162\n",
            "Epoch  53\n",
            "loss after epoch  19716.281805802817\n",
            "Epoch  54\n",
            "loss after epoch  19612.927655882737\n",
            "Epoch  55\n",
            "loss after epoch  19507.54433482495\n",
            "Epoch  56\n",
            "loss after epoch  19405.057150322067\n",
            "Epoch  57\n",
            "loss after epoch  19297.754469157153\n",
            "Epoch  58\n",
            "loss after epoch  19190.84857609679\n",
            "Epoch  59\n",
            "loss after epoch  19086.14041366959\n",
            "Epoch  60\n",
            "loss after epoch  18978.77808882346\n",
            "Epoch  61\n",
            "loss after epoch  18871.687552594813\n",
            "Epoch  62\n",
            "loss after epoch  18769.303800965045\n",
            "Epoch  63\n",
            "loss after epoch  18678.801229362103\n",
            "Epoch  64\n",
            "loss after epoch  18585.733071173418\n",
            "Epoch  65\n",
            "loss after epoch  18488.884209171305\n",
            "Epoch  66\n",
            "loss after epoch  18391.479892734835\n",
            "Epoch  67\n",
            "loss after epoch  18295.185895908096\n",
            "Epoch  68\n",
            "loss after epoch  18202.183296928983\n",
            "Epoch  69\n",
            "loss after epoch  18110.9358596155\n",
            "Epoch  70\n",
            "loss after epoch  18016.172860052764\n",
            "Epoch  71\n",
            "loss after epoch  17923.992823974182\n",
            "Epoch  72\n",
            "loss after epoch  17839.575580784895\n",
            "Epoch  73\n",
            "loss after epoch  17736.528265079254\n",
            "Epoch  74\n",
            "loss after epoch  17642.209399902258\n",
            "Epoch  75\n",
            "loss after epoch  17537.201150528483\n",
            "Epoch  76\n",
            "loss after epoch  17439.95393026074\n",
            "Epoch  77\n",
            "loss after epoch  17345.434428503297\n",
            "Epoch  78\n",
            "loss after epoch  17250.380249581503\n",
            "Epoch  79\n",
            "loss after epoch  17162.716588802054\n",
            "Epoch  80\n",
            "loss after epoch  17085.385279255162\n",
            "Epoch  81\n",
            "loss after epoch  17008.717725446048\n",
            "Epoch  82\n",
            "loss after epoch  16921.403069057695\n",
            "Epoch  83\n",
            "loss after epoch  16833.42575925476\n",
            "Epoch  84\n",
            "loss after epoch  16754.989257861544\n",
            "Epoch  85\n",
            "loss after epoch  16669.75449110245\n",
            "Epoch  86\n",
            "loss after epoch  16583.814222543602\n",
            "Epoch  87\n",
            "loss after epoch  16496.57832973113\n",
            "Epoch  88\n",
            "loss after epoch  16422.03071076087\n",
            "Epoch  89\n",
            "loss after epoch  16340.225613885652\n",
            "Epoch  90\n",
            "loss after epoch  16256.872988612324\n",
            "Epoch  91\n",
            "loss after epoch  16174.728411843627\n",
            "Epoch  92\n",
            "loss after epoch  16096.73997373734\n",
            "Epoch  93\n",
            "loss after epoch  16022.331137964537\n",
            "Epoch  94\n",
            "loss after epoch  15950.31651906149\n",
            "Epoch  95\n",
            "loss after epoch  15877.721152502801\n",
            "Epoch  96\n",
            "loss after epoch  15796.588280206317\n",
            "Epoch  97\n",
            "loss after epoch  15708.861275100044\n",
            "Epoch  98\n",
            "loss after epoch  15638.637838057595\n",
            "Epoch  99\n",
            "loss after epoch  15561.402026366384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpuXKyTzYJnB",
        "outputId": "da36443b-0959-4abd-90f4-13841243a79d"
      },
      "source": [
        "def predict(X_test,y_test):\n",
        "  _,_,y_hat=forwardPropagation(X_test,parameters)\n",
        "  y_hat=y_hat.argmax(axis=0)\n",
        "  correctPred=np.sum(y_hat==y_test)\n",
        "  print(\"Accuracy is \", correctPred/X_test.shape[0]*100,\"%\")\n",
        "  return (y_hat,correctPred)\n",
        "print(predict(X_train,y_train))\n",
        "print(predict(X_test,y_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  91.24833333333333 %\n",
            "(array([9, 0, 0, ..., 3, 0, 5]), 54749)\n",
            "Accuracy is  86.19 %\n",
            "(array([9, 2, 1, ..., 8, 1, 5]), 8619)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}