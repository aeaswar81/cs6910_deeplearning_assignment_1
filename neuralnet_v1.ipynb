{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assgnment1_DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVjuXsr1zLu2"
      },
      "source": [
        "#import the dataset \n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQLWjHat0lV4"
      },
      "source": [
        "#import the required libraries \n",
        "from matplotlib import pyplot\n",
        "import numpy as np"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP17UY7v0C7w"
      },
      "source": [
        "#load the data into train and test \n",
        "(X_train,y_train),(X_test,y_test)=fashion_mnist.load_data()\n",
        "#pyplot.imshow(X_train[2])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z27sdlQmMH1O",
        "outputId": "e1c667bd-56c1-4bfa-e6ca-dd68eac340bb"
      },
      "source": [
        "noOfImages = X_train.shape[0]\n",
        "X_train = (1.0/255)*np.array([X_train[i].flatten() for i in range(0,noOfImages)])\n",
        "X_train = np.array([X_train[i].flatten() for i in range(0,noOfImages)])\n",
        "noOftestImages= X_test.shape[0]\n",
        "X_test =(1.0/255)*np.array([X_test[i].flatten() for i in range(0,noOftestImages)])\n",
        "X_test =np.array([X_test[i].flatten() for i in range(0,noOftestImages)])\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtQEmgi3Opy_",
        "outputId": "e3d47804-f71c-425b-d273-7947ce87a7f6"
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
            " 0.         0.00392157 0.01568627 0.         0.         0.\n",
            " 0.         0.00392157 0.00392157 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.01176471 0.\n",
            " 0.14117647 0.53333333 0.49803922 0.24313725 0.21176471 0.\n",
            " 0.         0.         0.00392157 0.01176471 0.01568627 0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.02352941 0.         0.4        0.8\n",
            " 0.69019608 0.5254902  0.56470588 0.48235294 0.09019608 0.\n",
            " 0.         0.         0.         0.04705882 0.03921569 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
            " 0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
            " 0.30196078 0.50980392 0.28235294 0.05882353 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00392157 0.         0.27058824\n",
            " 0.81176471 0.8745098  0.85490196 0.84705882 0.84705882 0.63921569\n",
            " 0.49803922 0.4745098  0.47843137 0.57254902 0.55294118 0.34509804\n",
            " 0.6745098  0.25882353 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.00392157 0.00392157 0.         0.78431373 0.90980392 0.90980392\n",
            " 0.91372549 0.89803922 0.8745098  0.8745098  0.84313725 0.83529412\n",
            " 0.64313725 0.49803922 0.48235294 0.76862745 0.89803922 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
            " 0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
            " 0.8745098  0.96078431 0.67843137 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.75686275\n",
            " 0.89411765 0.85490196 0.83529412 0.77647059 0.70588235 0.83137255\n",
            " 0.82352941 0.82745098 0.83529412 0.8745098  0.8627451  0.95294118\n",
            " 0.79215686 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.01176471 0.         0.04705882 0.85882353 0.8627451  0.83137255\n",
            " 0.85490196 0.75294118 0.6627451  0.89019608 0.81568627 0.85490196\n",
            " 0.87843137 0.83137255 0.88627451 0.77254902 0.81960784 0.20392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.02352941 0.\n",
            " 0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
            " 0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
            " 0.96078431 0.46666667 0.65490196 0.21960784 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.01568627 0.         0.         0.21568627 0.9254902\n",
            " 0.89411765 0.90196078 0.89411765 0.94117647 0.90980392 0.83529412\n",
            " 0.85490196 0.8745098  0.91764706 0.85098039 0.85098039 0.81960784\n",
            " 0.36078431 0.         0.         0.         0.00392157 0.01568627\n",
            " 0.02352941 0.02745098 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.92941176 0.88627451 0.85098039 0.8745098\n",
            " 0.87058824 0.85882353 0.87058824 0.86666667 0.84705882 0.8745098\n",
            " 0.89803922 0.84313725 0.85490196 1.         0.30196078 0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.24313725 0.56862745 0.8\n",
            " 0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
            " 0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
            " 0.87843137 0.95686275 0.62352941 0.         0.         0.\n",
            " 0.         0.         0.07058824 0.17254902 0.32156863 0.41960784\n",
            " 0.74117647 0.89411765 0.8627451  0.87058824 0.85098039 0.88627451\n",
            " 0.78431373 0.80392157 0.82745098 0.90196078 0.87843137 0.91764706\n",
            " 0.69019608 0.7372549  0.98039216 0.97254902 0.91372549 0.93333333\n",
            " 0.84313725 0.         0.         0.22352941 0.73333333 0.81568627\n",
            " 0.87843137 0.86666667 0.87843137 0.81568627 0.8        0.83921569\n",
            " 0.81568627 0.81960784 0.78431373 0.62352941 0.96078431 0.75686275\n",
            " 0.80784314 0.8745098  1.         1.         0.86666667 0.91764706\n",
            " 0.86666667 0.82745098 0.8627451  0.90980392 0.96470588 0.\n",
            " 0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
            " 0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
            " 0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
            " 0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
            " 0.87058824 0.89411765 0.88235294 0.         0.38431373 0.91372549\n",
            " 0.77647059 0.82352941 0.87058824 0.89803922 0.89803922 0.91764706\n",
            " 0.97647059 0.8627451  0.76078431 0.84313725 0.85098039 0.94509804\n",
            " 0.25490196 0.28627451 0.41568627 0.45882353 0.65882353 0.85882353\n",
            " 0.86666667 0.84313725 0.85098039 0.8745098  0.8745098  0.87843137\n",
            " 0.89803922 0.11372549 0.29411765 0.8        0.83137255 0.8\n",
            " 0.75686275 0.80392157 0.82745098 0.88235294 0.84705882 0.7254902\n",
            " 0.77254902 0.80784314 0.77647059 0.83529412 0.94117647 0.76470588\n",
            " 0.89019608 0.96078431 0.9372549  0.8745098  0.85490196 0.83137255\n",
            " 0.81960784 0.87058824 0.8627451  0.86666667 0.90196078 0.2627451\n",
            " 0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
            " 0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
            " 0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
            " 0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
            " 0.70980392 0.80392157 0.80784314 0.45098039 0.         0.47843137\n",
            " 0.85882353 0.75686275 0.70196078 0.67058824 0.71764706 0.76862745\n",
            " 0.8        0.82352941 0.83529412 0.81176471 0.82745098 0.82352941\n",
            " 0.78431373 0.76862745 0.76078431 0.74901961 0.76470588 0.74901961\n",
            " 0.77647059 0.75294118 0.69019608 0.61176471 0.65490196 0.69411765\n",
            " 0.82352941 0.36078431 0.         0.         0.29019608 0.74117647\n",
            " 0.83137255 0.74901961 0.68627451 0.6745098  0.68627451 0.70980392\n",
            " 0.7254902  0.7372549  0.74117647 0.7372549  0.75686275 0.77647059\n",
            " 0.8        0.81960784 0.82352941 0.82352941 0.82745098 0.7372549\n",
            " 0.7372549  0.76078431 0.75294118 0.84705882 0.66666667 0.\n",
            " 0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
            " 0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
            " 0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
            " 0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
            " 0.38823529 0.22745098 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.15686275\n",
            " 0.23921569 0.17254902 0.28235294 0.16078431 0.1372549  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFA10HAHKCb8"
      },
      "source": [
        "def softmax(X):\n",
        "  X=np.exp(X)\n",
        "  sum=np.sum(X,axis=0)\n",
        "  return X/sum \n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZgtMpEAUkIA"
      },
      "source": [
        "def sigmoidFunc(X):\n",
        "\n",
        "  return 1.0/(1.0+np.exp(-X))\n",
        "  #return res"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GJHo_F55wKD"
      },
      "source": [
        "def gDash(X):\n",
        "  return sigmoidFunc(X)*(1-sigmoidFunc(X))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIfFadrLAkmM"
      },
      "source": [
        "def forwardPropagation(X,parameters):\n",
        "  preactivation={}\n",
        "  activation={}\n",
        "  activation['h0']=X.T\n",
        "  #print(activation['h0'].shape)\n",
        "  for k in range(1,noOfHiddenLayers+1):\n",
        "    preactivation['a'+str(k)]=np.dot(parameters['W'+str(k)],activation['h'+str(k-1)])+parameters['b'+str(k)]\n",
        "    activation['h'+str(k)]=sigmoidFunc(preactivation['a'+str(k)])\n",
        "    #print('h size '+str(k),activation['h'+str(k)].shape)\n",
        "    #print('a'+str(k),preactivation['a'+str(k)])\n",
        "  preactivation['a'+str(noOfHiddenLayers+1)]=np.dot(parameters['W'+str(noOfHiddenLayers+1)],activation['h'+str(noOfHiddenLayers)])+parameters['b'+str(noOfHiddenLayers+1)]\n",
        "  y=softmax(preactivation['a'+str(noOfHiddenLayers+1)])    \n",
        "  #print(\"a last\" ,preactivation['a'+str(noOfHiddenLayers+1)])\n",
        "  return (preactivation,activation,y)\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZFsUg0xrJtv"
      },
      "source": [
        "def backPropagation(parameters,activation,preactivation,yhat,X,y_train):\n",
        "  grads={}\n",
        "  eIndicator=np.zeros((10,X.shape[0]))\n",
        "  eIndicator[y_train,np.arange(X.shape[0])]=1\n",
        "  #print(y_train)\n",
        "  #print(eIndicator)\n",
        "  #print(eIndicator.shape)\n",
        "  grads['a'+str(noOfHiddenLayers+1)]= -(eIndicator-yhat)\n",
        "  #print(grads['a'+str(noOfHiddenLayers+1)].shape)\n",
        "  for j in range(noOfHiddenLayers+1,0,-1):\n",
        "    grads['W'+str(j)]= np.dot(grads['a'+str(j)],activation['h'+str(j-1)].T)\n",
        "    #grads['W'+str(j)]= np.dot(activation['h'+str(j-1)],grads['a'+str(j)].T).T\n",
        "    #print(grads['W'+str(j)].shape)\n",
        "    grads['b'+str(j)]= np.sum(grads['a'+str(j)],axis=1,keepdims=True)\n",
        "    ###grads['h'+str(j-1)]=np.dot(grads['W'+str(j)].T,grads['a'+str(j)]) #error\n",
        "    grads['h'+str(j-1)]=np.dot(parameters['W'+str(j)].T,grads['a'+str(j)])\n",
        "    #print('h'+str(j-1),grads['h'+str(j-1)].shape)\n",
        "    #print('a'+str(j-1),preactivation['a'+str(j-1)].shape)\n",
        "    if (j!=1):\n",
        "      grads['a'+str(j-1)]=grads['h'+str(j-1)]*gDash(preactivation['a'+str(j-1)])\n",
        "  return grads\n",
        "#backPropagation()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MXQ17Gk_Cuv"
      },
      "source": [
        "def Loss(yhat,y_train,X):\n",
        "  eIndicator=np.zeros((10,X.shape[0]))\n",
        "  eIndicator[y_train,np.arange(X.shape[0])]=1\n",
        "  eIndicator=eIndicator*yhat\n",
        "  eIndicator=eIndicator.sum(axis=0)\n",
        "  eIndicator=np.log(eIndicator)\n",
        "  return -sum(eIndicator)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnJHWPgMiiLw",
        "outputId": "f168bf96-7bec-4169-8f36-663dd5b901b3"
      },
      "source": [
        "\n",
        "l=10 #output classes\n",
        "noOfneuronsEach=[120,20]\n",
        "noOfHiddenLayers=len(noOfneuronsEach)\n",
        "inputNeuronSize=X_train.shape[1]\n",
        "print(inputNeuronSize)\n",
        "parameters={}\n",
        "batchSize=600\n",
        "iterations=100\n",
        "eta=1\n",
        "#eta=0.1\n",
        "#input W\n",
        "#initialization\n",
        "parameters['W'+str(1)] =np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[0], inputNeuronSize))\n",
        "parameters['b'+str(1)]= np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[0],1))\n",
        "#parameters['W'+str(1)] =0+1.5*np.random.randn(noOfneuronsEach[0], inputNeuronSize)\n",
        "#parameters['b'+str(1)]= 0+1.5*np.random.randn(noOfneuronsEach[0],1)\n",
        "for i in range(2,noOfHiddenLayers+1):\n",
        "  parameters['W'+str(i)] = np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "  parameters['b'+str(i)]= np.random.uniform(low=-0.5,high=0.5,size=(noOfneuronsEach[i-1],1))\n",
        "  #parameters['W'+str(i)] = 0+1.5*np.random.randn(noOfneuronsEach[i-1], noOfneuronsEach[i-2])\n",
        "  #parameters['b'+str(i)]= 0+1.5*np.random.randn(noOfneuronsEach[i-1],1)\n",
        "#Output W\n",
        "parameters['W'+str(noOfHiddenLayers+1)] = np.random.uniform(low=-0.5,high=0.5,size=(l, noOfneuronsEach[-1]))\n",
        "parameters['b'+str(noOfHiddenLayers+1)]= np.random.uniform(low=-0.5,high=0.5,size=(l,1))\n",
        "#parameters['W'+str(noOfHiddenLayers+1)] = 0+1.5*np.random.randn(l, noOfneuronsEach[-1])\n",
        "#parameters['b'+str(noOfHiddenLayers+1)]= 0+1.5*np.random.randn(l,1)\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "3rIfc41ocC7-",
        "outputId": "7474480f-d8f8-4280-8410-8da51b818be4"
      },
      "source": [
        "def vanillagradDescent():\n",
        "  t=0\n",
        "  while(t < iterations):\n",
        "    print(\"Epoch \",t)\n",
        "    #print(parameters)\n",
        "    mini=0\n",
        "    while(mini<(noOfImages/batchSize)):\n",
        "      #print(\"Batch \",mini)\n",
        "      X_mini=X_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      y_mini=y_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      preactivation,activation,yhat=forwardPropagation(X_mini,parameters)\n",
        "      gradients=backPropagation(parameters,activation,preactivation,yhat,X_mini,y_mini)\n",
        "    #print(\"gradients\",gradients)\n",
        "      for i in range(1,noOfHiddenLayers+2):\n",
        "        parameters['W'+str(i)] -=  eta*(1.0/X_mini.shape[0])*gradients['W'+str(i)]#parameters['W'+str(i)] -=  eta*(1.0/X_train.shape[0])*gradients['W'+str(i)]\n",
        "        parameters['b'+str(i)] -= eta*(1.0/X_mini.shape[0])*gradients['b'+str(i)]#parameters['b'+str(i)] -= eta*(1.0/X_train.shape[0])*gradients['b'+str(i)]\n",
        "      mini+=1\n",
        "    _,_,yhat=forwardPropagation(X_train,parameters)\n",
        "    print(\"loss after epoch \",Loss(yhat,y_train,X_train))\n",
        "    \n",
        "    #print(yhat)\n",
        "    t+=1\n",
        "vanillagradDescent()\n",
        "#print(parameters)\n",
        "#print(gradients)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "loss after epoch  47573.393426219605\n",
            "Epoch  1\n",
            "loss after epoch  41904.39835775272\n",
            "Epoch  2\n",
            "loss after epoch  33197.24226676189\n",
            "Epoch  3\n",
            "loss after epoch  30161.114459625456\n",
            "Epoch  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-1fc27ecb55d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#print(yhat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mvanillagradDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(parameters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#print(gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-1fc27ecb55d6>\u001b[0m in \u001b[0;36mvanillagradDescent\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mX_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0my_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#print(\"gradients\",gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-52e34919ac23>\u001b[0m in \u001b[0;36mforwardPropagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(activation['h0'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoOfHiddenLayers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoidFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print('h size '+str(k),activation['h'+str(k)].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "5PBJIPYtcLmg",
        "outputId": "c05fcfd0-1cc9-4643-9ce8-bbfa92315a15"
      },
      "source": [
        "def momentumGD():\n",
        "  #initialization\n",
        "  prevW={}\n",
        "  prevb={}\n",
        "  gamma=0.9\n",
        "  prevW['W'+str(1)] =np.zeros((noOfneuronsEach[0], inputNeuronSize))\n",
        "  prevb['b'+str(1)]= np.zeros((noOfneuronsEach[0],1))\n",
        "  for i in range(2,noOfHiddenLayers+1):\n",
        "    prevW['W'+str(i)] = np.zeros((noOfneuronsEach[i-1], noOfneuronsEach[i-2]))\n",
        "    prevb['b'+str(i)]= np.zeros((noOfneuronsEach[i-1],1))\n",
        "  prevW['W'+str(noOfHiddenLayers+1)] = np.zeros((l, noOfneuronsEach[-1]))\n",
        "  prevb['b'+str(noOfHiddenLayers+1)]= np.zeros((l,1))\n",
        "  t=0\n",
        "  while(t<iterations):\n",
        "    print(\"Epoch \",t)\n",
        "    mini=0\n",
        "    while(mini<(noOfImages/batchSize)):\n",
        "      #print(\"Batch \",mini)\n",
        "      X_mini=X_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      y_mini=y_train[(mini*batchSize):((mini+1)*batchSize-1)]\n",
        "      preactivation,activation,yhat=forwardPropagation(X_mini,parameters)\n",
        "      gradients=backPropagation(parameters,activation,preactivation,yhat,X_mini,y_mini)\n",
        "    #print(\"gradients\",gradients)\n",
        "      for i in range(1,noOfHiddenLayers+2):\n",
        "        w=gamma*prevW['W'+str(i)]+eta*(1.0/X_mini.shape[0])*gradients['W'+str(i)]\n",
        "        b=gamma*prevb['b'+str(i)]+eta*(1.0/X_mini.shape[0])*gradients['b'+str(i)]\n",
        "        parameters['W'+str(i)] -=  w\n",
        "        parameters['b'+str(i)] -=  b\n",
        "        prevW['W'+str(i)]=w\n",
        "        prevb['b'+str(i)]=b\n",
        "      mini+=1\n",
        "    _,_,yhat=forwardPropagation(X_train,parameters)\n",
        "    print(\"loss after epoch \",Loss(yhat,y_train,X_train))\n",
        "    \n",
        "    #print(yhat)\n",
        "    t+=1\n",
        "momentumGD()\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "loss after epoch  28235.31184225386\n",
            "Epoch  1\n",
            "loss after epoch  23913.73563627984\n",
            "Epoch  2\n",
            "loss after epoch  22276.422719371847\n",
            "Epoch  3\n",
            "loss after epoch  20752.77516627167\n",
            "Epoch  4\n",
            "loss after epoch  19774.4372024492\n",
            "Epoch  5\n",
            "loss after epoch  18429.12712121991\n",
            "Epoch  6\n",
            "loss after epoch  17735.588868695533\n",
            "Epoch  7\n",
            "loss after epoch  17099.309072066786\n",
            "Epoch  8\n",
            "loss after epoch  17417.51882520029\n",
            "Epoch  9\n",
            "loss after epoch  16623.978858659517\n",
            "Epoch  10\n",
            "loss after epoch  16181.079187361598\n",
            "Epoch  11\n",
            "loss after epoch  17144.966061575873\n",
            "Epoch  12\n",
            "loss after epoch  16605.78465978326\n",
            "Epoch  13\n",
            "loss after epoch  15256.996168583055\n",
            "Epoch  14\n",
            "loss after epoch  14760.010781849554\n",
            "Epoch  15\n",
            "loss after epoch  14268.573224938957\n",
            "Epoch  16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-e9e625fe3bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#print(yhat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmomentumGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-e9e625fe3bc2>\u001b[0m in \u001b[0;36mmomentumGD\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mX_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0my_mini\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#print(\"gradients\",gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-52e34919ac23>\u001b[0m in \u001b[0;36mforwardPropagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(activation['h0'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoOfHiddenLayers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoidFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print('h size '+str(k),activation['h'+str(k)].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpuXKyTzYJnB",
        "outputId": "055812c8-4fda-4b57-f097-25a44ce658ba"
      },
      "source": [
        "def predict(X_test,y_test):\n",
        "  _,_,y_hat=forwardPropagation(X_test,parameters)\n",
        "  y_hat=y_hat.argmax(axis=0)\n",
        "  correctPred=np.sum(y_hat==y_test)\n",
        "  print(\"Accuracy is \", correctPred/X_test.shape[0]*100,\"%\")\n",
        "  return (y_hat,correctPred)\n",
        "print(predict(X_train,y_train))\n",
        "print(predict(X_test,y_test))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  93.455 %\n",
            "(array([9, 0, 0, ..., 3, 0, 5]), 56073)\n",
            "Accuracy is  87.31 %\n",
            "(array([9, 2, 1, ..., 8, 1, 5]), 8731)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}